{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Direction Prediction - Dataset Generation\n",
    "\n",
    "This notebook generates two datasets for predicting stock price direction:\n",
    "1. **Basic Dataset**: Raw OHLCV data with binary target\n",
    "2. **Enhanced Dataset**: OHLCV data + technical indicators with binary target\n",
    "\n",
    "## Setup and Data Collection\n",
    "- Tickers: Top 50 S&P 500 stocks by market cap\n",
    "- Date Range: 2010-01-01 to 2024-12-31\n",
    "- Train/Test Split: 2010-2021 (Train), 2022-2024 (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas methods used in this notebook — quick reference\n",
    "\n",
    "Below are concise, actionable descriptions of the DataFrame/Series methods and operations that appear in this notebook. Each item shows what it does and a brief example of the effect.\n",
    "\n",
    "- shift(n)\n",
    "  - Moves values up/down along the index. Commonly used to compare current row with next/previous row.\n",
    "  - Example: df['Close'].shift(-1) gives next-day close.\n",
    "\n",
    "- astype(type)\n",
    "  - Casts dtype of a Series (or DataFrame) to the given type.\n",
    "  - Example: (next_close > df['Close']).astype(int) converts boolean to 0/1.\n",
    "\n",
    "- df.columns (assignment)\n",
    "  - Replace column labels. Used here to flatten MultiIndex columns by joining parts or taking the last part.\n",
    "  - Example: df.columns = [f\"{p}_{t}\" for p,t in df.columns]\n",
    "\n",
    "- isinstance(df.columns, pd.MultiIndex)\n",
    "  - Checks whether columns have multiple levels (e.g., (Price, Ticker)). If true, you may want to flatten or stack.\n",
    "\n",
    "- get_level_values(level)\n",
    "  - Returns the labels for a specific MultiIndex level (useful to find empty tickers or particular level values).\n",
    "  - Example: df.columns.get_level_values('Ticker')\n",
    "\n",
    "- stack(level)\n",
    "  - Moves one column-level into the row index, turning wide MultiIndex columns into long rows. Use when you want a tidy table with a Ticker column.\n",
    "  - Example: df.stack(level='Ticker') → index becomes (original_index, Ticker) and price labels stay as columns.\n",
    "\n",
    "- reset_index()\n",
    "  - Converts index levels into columns and resets index to default integer index.\n",
    "  - Often used after stack() to get a flat DataFrame: df.stack(...).reset_index()\n",
    "\n",
    "- sort_values(by)\n",
    "  - Sorts rows by one or more columns. Important before groupby operations that assume ordering.\n",
    "  - Example: df.sort_values(['Ticker','Date'])\n",
    "\n",
    "- reset_index(drop=True)\n",
    "  - Reset index and drop the old index instead of adding it as a column.\n",
    "\n",
    "- groupby(...).apply(func)\n",
    "  - Group rows and apply a function to each group. Useful for per-ticker operations, e.g., remove last row for each ticker: df.groupby('Ticker').apply(lambda x: x.iloc[:-1])\n",
    "\n",
    "- iloc / loc\n",
    "  - iloc: integer-location based selection. loc: label-based selection. Both used to select rows/columns precisely.\n",
    "  - Example: x.iloc[:-1] takes all but the last row of group x. dataset2.loc[mask, 'SMA_10'] assigns values to rows matching mask.\n",
    "\n",
    "- unique()\n",
    "  - Returns unique values in a Series (e.g., list of tickers).\n",
    "\n",
    "- isnull().sum()\n",
    "  - Counts missing values per column.\n",
    "\n",
    "- dropna()\n",
    "  - Drops rows (or columns) containing NA. Here used to remove rows missing indicator values after rolling/ewm computations.\n",
    "\n",
    "- rolling(window).mean()\n",
    "  - Computes rolling statistics (SMA) over a window of rows.\n",
    "  - Example: df['Close'].rolling(10).mean() gives 10-day SMA.\n",
    "\n",
    "- diff()\n",
    "  - First discrete difference on Series: current - previous. Used to compute gains/losses for RSI.\n",
    "\n",
    "- where(condition, other)\n",
    "  - Keeps values where condition True, else replaces with other. Useful to separate gains (positive diffs) from losses.\n",
    "\n",
    "- ewm(span, adjust=False).mean()\n",
    "  - Exponential weighted mean (EMA). Used for MACD and signal smoothing.\n",
    "\n",
    "- drop_duplicates()\n",
    "  - Remove duplicate rows (not used heavily here but commonly used when combining datasets).\n",
    "\n",
    "- to_csv / to_parquet\n",
    "  - Persist DataFrame to disk in CSV or Parquet formats.\n",
    "\n",
    "- melt(id_vars, var_name, value_name)\n",
    "  - Unpivot a DataFrame from wide to long format. Use when you flattened column names like 'Close_AAPL' and want rows per ticker.\n",
    "  - Example: df.reset_index().melt(id_vars='Date')\n",
    "\n",
    "- str.rsplit(sep, n=1, expand=True)\n",
    "  - Split string column from right into parts. Useful to split 'Close_AAPL' into ['Close','AAPL'].\n",
    "\n",
    "- copy()\n",
    "  - Create an explicit copy of a DataFrame to avoid modifying the original view in-place.\n",
    "\n",
    "- head(n) / shape / columns / value_counts() / mean()\n",
    "  - Utility methods: head shows top rows, shape gives (rows, cols), columns lists column names, value_counts gives counts per value, mean gives average (useful for target balance).\n",
    "\n",
    "Tip: for tidy machine learning datasets prefer the long form (one row per Date+Ticker) produced by stacking or melting — columns: Date, Ticker, Open, High, Low, Close, Volume, indicators..., Target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get Top 50 S&P 500 Stocks by Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AAPL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AMZN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GOOGL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading BRK-B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading JPM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading JNJ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading V...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading UNH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data download complete.\n",
      "Shape: (14740, 52)\n",
      "Price        Date      Close       High        Low       Open       Volume  \\\n",
      "Ticker                  AAPL       AAPL       AAPL       AAPL         AAPL   \n",
      "0      2020-01-02  72.468254  72.528574  71.223252  71.476592  135480400.0   \n",
      "1      2020-01-03  71.763733  72.523762  71.539345  71.696175  146322800.0   \n",
      "2      2020-01-06  72.335564  72.374169  70.634547  70.885479  118387200.0   \n",
      "3      2020-01-07  71.995361  72.600968  71.775796  72.345212  108872000.0   \n",
      "4      2020-01-08  73.153488  73.455087  71.698574  71.698574  132079200.0   \n",
      "\n",
      "Price  Ticker Close High  Low  ... Close High Low Open Volume Close High Low  \\\n",
      "Ticker         MSFT MSFT MSFT  ...     V    V   V    V      V   UNH  UNH UNH   \n",
      "0        AAPL   NaN  NaN  NaN  ...   NaN  NaN NaN  NaN    NaN   NaN  NaN NaN   \n",
      "1        AAPL   NaN  NaN  NaN  ...   NaN  NaN NaN  NaN    NaN   NaN  NaN NaN   \n",
      "2        AAPL   NaN  NaN  NaN  ...   NaN  NaN NaN  NaN    NaN   NaN  NaN NaN   \n",
      "3        AAPL   NaN  NaN  NaN  ...   NaN  NaN NaN  NaN    NaN   NaN  NaN NaN   \n",
      "4        AAPL   NaN  NaN  NaN  ...   NaN  NaN NaN  NaN    NaN   NaN  NaN NaN   \n",
      "\n",
      "Price  Open Volume  \n",
      "Ticker  UNH    UNH  \n",
      "0       NaN    NaN  \n",
      "1       NaN    NaN  \n",
      "2       NaN    NaN  \n",
      "3       NaN    NaN  \n",
      "4       NaN    NaN  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# List of top 50 tickers\n",
    "top_50_tickers = [\n",
    "    'AAPL','MSFT','AMZN','GOOGL','BRK-B','JPM','JNJ','PG','V','UNH'\n",
    "]\n",
    "\n",
    "# Fix for Yahoo Finance tickers with dashes\n",
    "top_50_tickers = [ticker.replace('-', '-') for ticker in top_50_tickers]\n",
    "\n",
    "# Download data for all tickers\n",
    "all_data = []\n",
    "\n",
    "for ticker in top_50_tickers:\n",
    "    try:\n",
    "        print(f\"Downloading {ticker}...\")\n",
    "        df = yf.download(ticker, start=\"2020-01-01\", end=\"2025-11-12\")\n",
    "        df['Ticker'] = ticker\n",
    "        all_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {ticker}: {e}\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "combined_df = pd.concat(all_data)\n",
    "combined_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"✅ Data download complete.\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Replace Wikipedia scraping with a fixed list of 10 tickers public since 2010\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading historical data from 2010-01-01 to 2024-12-31...\n",
      "\n",
      "Downloading AAPL... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading MSFT... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading AMZN... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading GOOGL... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading BRK-B... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading JPM... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading JNJ... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading PG... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading V... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n",
      "Downloading UNH... OK (3773 rows)\n",
      "Index(['Close', 'High', 'Low', 'Open', 'Volume', 'Target'], dtype='object', name='Price')\n"
     ]
    }
   ],
   "source": [
    "# Define date ranges\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "print(f\"Downloading historical data from {start_date} to {end_date}...\\n\")\n",
    "\n",
    "# Download data for all tickers\n",
    "all_data = []\n",
    "\n",
    "if not top_50_tickers:\n",
    "    raise RuntimeError(\"No tickers available to download. Check the Wikipedia scraping step.\")\n",
    "\n",
    "for ticker in top_50_tickers:\n",
    "    try:\n",
    "        print(f\"Downloading {ticker}...\", end=' ')\n",
    "        \n",
    "        df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        \n",
    "        \n",
    "        if not df.empty:\n",
    "            next_close = df['Close'].shift(-1)\n",
    "            df['Target'] = (next_close > df['Close']).astype(int)\n",
    "            \n",
    "            \n",
    "            all_data.append(df)\n",
    "            print(f\"OK ({len(df)} rows)\")\n",
    "            \n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = df.columns.droplevel('Ticker')\n",
    "                print(df.columns)\n",
    "        else:\n",
    "            print(\"No data available\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    time.sleep(0.5)  # be polite to the data provider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data ---------------------------------------------\n",
      "Price            Close        High         Low        Open     Volume  Target\n",
      "Date                                                                         \n",
      "2010-01-04    6.418384    6.433080    6.369499    6.400989  493729600       1\n",
      "2010-01-05    6.429481    6.465770    6.395590    6.436079  601904800       0\n",
      "2010-01-06    6.327211    6.454973    6.320613    6.429481  552160000       0\n",
      "2010-01-07    6.315513    6.358101    6.269627    6.350603  477131200       1\n",
      "2010-01-08    6.357502    6.358103    6.269929    6.307118  447610800       0\n",
      "...                ...         ...         ...         ...        ...     ...\n",
      "2024-12-23  254.120697  254.498976  252.308884  253.622948   40858800       1\n",
      "2024-12-24  257.037476  257.047410  254.140559  254.339671   23234700       1\n",
      "2024-12-26  257.853760  258.928914  256.470034  257.027510   27237100       0\n",
      "2024-12-27  254.439224  257.535238  251.920617  256.669129   42355300       0\n",
      "2024-12-30  251.064499  252.358649  249.621030  251.094363   35557500       0\n",
      "\n",
      "[3773 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine all data\n",
    "if not all_data:\n",
    "    raise RuntimeError(\"No data was downloaded for any tickers. Aborting.\")\n",
    "\n",
    "\n",
    "raw_data = all_data\n",
    "print('raw_data ---------------------------------------------')\n",
    "print(raw_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset 1 - Basic OHLCV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 (Basic) created\n",
      "Price          Close      High       Low      Open     Volume  Target\n",
      "Date                                                                 \n",
      "2010-01-04  6.418384  6.433080  6.369499  6.400989  493729600       1\n",
      "2010-01-05  6.429481  6.465770  6.395590  6.436079  601904800       0\n",
      "2010-01-06  6.327211  6.454973  6.320613  6.429481  552160000       0\n",
      "2010-01-07  6.315513  6.358101  6.269627  6.350603  477131200       1\n",
      "2010-01-08  6.357502  6.358103  6.269929  6.307118  447610800       0\n",
      "2010-01-11  6.301419  6.388092  6.251633  6.382094  462229600       0\n",
      "2010-01-12  6.229738  6.291220  6.190750  6.273825  594459600       1\n",
      "2010-01-13  6.317612  6.326010  6.121172  6.234238  605892000       0\n",
      "2010-01-14  6.281024  6.311916  6.268728  6.301419  432894000       0\n",
      "2010-01-15  6.176055  6.346105  6.174256  6.326010  594067600       1\n"
     ]
    }
   ],
   "source": [
    "# Dataset 1: Basic OHLCV + Target\n",
    "dataset1 = [data.copy(deep=True) for data in raw_data]\n",
    "print(f\"Dataset 1 (Basic) created\")\n",
    "print(dataset1[0].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Technical Indicators for Dataset 2\n",
    "\n",
    "Technical indicators to calculate:\n",
    "- **Simple Moving Averages (SMA)**: 10-day, 50-day, 200-day\n",
    "- **Relative Strength Index (RSI)**: 14-day period\n",
    "- **MACD**: 12, 26, 9 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical indicator functions defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_sma(data, window):\n",
    "    \"\"\"Calculate Simple Moving Average\"\"\"\n",
    "    return data.rolling(window=window).mean()\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD and Signal line\"\"\"\n",
    "    ema_fast = data.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = data.ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd, signal_line\n",
    "\n",
    "print(\"Technical indicator functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating technical indicators for each ticker...\n",
      "\n",
      "Price          Close      High       Low      Open      Volume  Target  \\\n",
      "Date                                                                     \n",
      "2010-10-18  9.537151  9.567141  9.425884  9.551247  1093010800       0   \n",
      "2010-10-19  9.281930  9.410291  8.997915  9.099285  1232784000       1   \n",
      "2010-10-20  9.313118  9.424685  9.203351  9.267232   721624400       0   \n",
      "2010-10-21  9.282829  9.439382  9.201253  9.368003   551460000       0   \n",
      "2010-10-22  9.221343  9.298420  9.186254  9.269328   372778000       1   \n",
      "\n",
      "Price         SMA-10    SMA-50   SMA-200        RSI      MACD  Signal_Line  \n",
      "Date                                                                        \n",
      "2010-10-18  8.968852  8.111438  7.362248  82.037181  0.321097     0.263761  \n",
      "2010-10-19  9.030484  8.140073  7.376566  69.540844  0.321368     0.275283  \n",
      "2010-10-20  9.094485  8.170736  7.390984  74.787042  0.320406     0.284307  \n",
      "2010-10-21  9.155367  8.206324  7.405762  75.093017  0.313584     0.290163  \n",
      "2010-10-22  9.195554  8.239722  7.420291  77.737082  0.299762     0.292082  \n",
      "Price           Close       High        Low       Open    Volume  Target  \\\n",
      "Date                                                                       \n",
      "2010-10-18  19.614458  19.713215  19.333384  19.439736  48330500       0   \n",
      "2010-10-19  19.067495  19.272604  18.953546  19.196638  66150900       1   \n",
      "2010-10-20  19.227013  19.295382  19.067485  19.189030  56283600       1   \n",
      "2010-10-21  19.310589  19.401749  19.029514  19.295395  50032400       0   \n",
      "2010-10-22  19.280207  19.401754  19.196645  19.386560  25837900       0   \n",
      "\n",
      "Price          SMA-10     SMA-50    SMA-200        RSI      MACD  Signal_Line  \n",
      "Date                                                                           \n",
      "2010-10-18  18.933039  18.651657  20.480073  69.655150  0.162363     0.057888  \n",
      "2010-10-19  18.990014  18.645973  20.459543  58.720793  0.154604     0.077232  \n",
      "2010-10-20  19.056863  18.651640  20.439774  61.263517  0.159489     0.093683  \n",
      "2010-10-21  19.124472  18.662152  20.421134  64.285488  0.168165     0.108580  \n",
      "2010-10-22  19.186005  18.677648  20.403541  72.897303  0.170623     0.120988  \n"
     ]
    }
   ],
   "source": [
    "# Create Dataset 2 with technical indicators\n",
    "dataset2 = [data.copy(deep=True) for data in raw_data]\n",
    "\n",
    "print(\"Calculating technical indicators for each ticker...\\n\")\n",
    "\n",
    "for i in range(len(dataset2)):\n",
    "    ticker_data = dataset2[i]\n",
    "    ticker_data['SMA-10'] = calculate_sma(ticker_data['Close'], 10)\n",
    "    ticker_data['SMA-50'] = calculate_sma(ticker_data['Close'], 50)\n",
    "    ticker_data['SMA-200'] = calculate_sma(ticker_data['Close'], 200)\n",
    "    \n",
    "    ticker_data['RSI'] = calculate_rsi(ticker_data['Close'])\n",
    "    \n",
    "    ticker_data['MACD'], ticker_data['Signal_Line'] = calculate_macd(ticker_data['Close'])\n",
    "\n",
    "    dataset2[i] = ticker_data.dropna()\n",
    "    \n",
    "    print(dataset2[i].head(5))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clean Data - Drop Rows with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Split Data into Train and Test Sets\n",
    "\n",
    "- **Training**: 2010-2021\n",
    "- **Testing**: 2022-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price          Close      High       Low      Open     Volume  Target\n",
      "Date                                                                 \n",
      "2010-01-04  6.418384  6.433080  6.369499  6.400989  493729600       1\n",
      "2010-01-05  6.429480  6.465769  6.395590  6.436078  601904800       0\n",
      "2010-01-06  6.327211  6.454973  6.320613  6.429481  552160000       0\n",
      "2010-01-07  6.315513  6.358101  6.269627  6.350603  477131200       1\n",
      "2010-01-08  6.357501  6.358101  6.269928  6.307116  447610800       0\n",
      "Price          Close      High       Low      Open      Volume  Target  \\\n",
      "Date                                                                     \n",
      "2010-10-18  9.537151  9.567141  9.425884  9.551247  1093010800       0   \n",
      "2010-10-19  9.281930  9.410291  8.997915  9.099285  1232784000       1   \n",
      "2010-10-20  9.313118  9.424685  9.203351  9.267232   721624400       0   \n",
      "2010-10-21  9.282829  9.439382  9.201253  9.368003   551460000       0   \n",
      "2010-10-22  9.221343  9.298420  9.186254  9.269328   372778000       1   \n",
      "\n",
      "Price         SMA-10    SMA-50   SMA-200        RSI      MACD  Signal_Line  \n",
      "Date                                                                        \n",
      "2010-10-18  8.968852  8.111438  7.362248  82.037181  0.321097     0.263761  \n",
      "2010-10-19  9.030484  8.140073  7.376566  69.540844  0.321368     0.275283  \n",
      "2010-10-20  9.094485  8.170736  7.390984  74.787042  0.320406     0.284307  \n",
      "2010-10-21  9.155367  8.206324  7.405762  75.093017  0.313584     0.290163  \n",
      "2010-10-22  9.195554  8.239722  7.420291  77.737082  0.299762     0.292082  \n",
      "============================================================\n",
      "Dataset 1 (Basic OHLCV)\n",
      "============================================================\n",
      "Training set: 6,042 rows (2010-01-04 00:00:00 to 2021-12-31 00:00:00)\n",
      "Testing set:  1,504 rows (2022-01-03 00:00:00 to 2024-12-30 00:00:00)\n",
      "\n",
      "Target distribution - Train: 52.83% positive\n",
      "Target distribution - Test:  52.39% positive\n",
      "\n",
      "============================================================\n",
      "Dataset 2 (With Technical Indicators)\n",
      "============================================================\n",
      "Training set: 5,644 rows (2010-10-18 00:00:00 to 2021-12-31 00:00:00)\n",
      "Testing set:  1,504 rows (2022-01-03 00:00:00 to 2024-12-30 00:00:00)\n",
      "\n",
      "Target distribution - Train: 52.71% positive\n",
      "Target distribution - Test:  52.39% positive\n"
     ]
    }
   ],
   "source": [
    "# Define split date\n",
    "split_date = '2022-01-01'\n",
    "\n",
    "# Combine the DFs\n",
    "\n",
    "dataset1_full = pd.concat(dataset1)\n",
    "\n",
    "\n",
    "dataset2_full = pd.concat(dataset2)\n",
    "\n",
    "print(dataset1_full.head(5))\n",
    "print(dataset2_full.head(5))\n",
    "\n",
    "# Split Dataset 1\n",
    "\n",
    "dataset1_train = dataset1_full[dataset1_full.index < split_date].copy()\n",
    "dataset1_test = dataset1_full[dataset1_full.index >= split_date].copy()\n",
    "\n",
    "# Split Dataset 2\n",
    "dataset2_train = dataset2_full[dataset2_full.index < split_date].copy()\n",
    "dataset2_test = dataset2_full[dataset2_full.index >= split_date].copy()\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset 1 (Basic OHLCV)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set: {dataset1_train.shape[0]:,} rows ({dataset1_train.index.min()} to {dataset1_train.index.max()})\")\n",
    "print(f\"Testing set:  {dataset1_test.shape[0]:,} rows ({dataset1_test.index.min()} to {dataset1_test.index.max()})\")\n",
    "print(f\"\\nTarget distribution - Train: {dataset1_train['Target'].mean()*100:.2f}% positive\")\n",
    "print(f\"Target distribution - Test:  {dataset1_test['Target'].mean()*100:.2f}% positive\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset 2 (With Technical Indicators)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set: {dataset2_train.shape[0]:,} rows ({dataset2_train.index.min()} to {dataset2_train.index.max()})\")\n",
    "print(f\"Testing set:  {dataset2_test.shape[0]:,} rows ({dataset2_test.index.min()} to {dataset2_test.index.max()})\")\n",
    "print(f\"\\nTarget distribution - Train: {dataset2_train['Target'].mean()*100:.2f}% positive\")\n",
    "print(f\"Target distribution - Test:  {dataset2_test['Target'].mean()*100:.2f}% positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Datasets to CSV and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Dataset 1 (Basic OHLCV)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m dataset1_train.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/dataset1_train.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m dataset1_test.to_csv(\u001b[33m'\u001b[39m\u001b[33m../data/dataset1_test.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdataset1_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/dataset1_train.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m dataset1_test.to_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/dataset1_test.parquet\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Dataset 1 saved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ger85847\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ger85847\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ger85847\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ger85847\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Save Dataset 1 (Basic)\n",
    "print(\"Saving Dataset 1 (Basic OHLCV)...\")\n",
    "dataset1_train.to_csv('../data/dataset1_train.csv', index=False)\n",
    "dataset1_test.to_csv('../data/dataset1_test.csv', index=False)\n",
    "dataset1_train.to_parquet('../data/dataset1_train.parquet', index=False)\n",
    "dataset1_test.to_parquet('../data/dataset1_test.parquet', index=False)\n",
    "print(\"âœ“ Dataset 1 saved\")\n",
    "\n",
    "# Save Dataset 2 (Enhanced)\n",
    "print(\"\\nSaving Dataset 2 (With Technical Indicators)...\")\n",
    "dataset2_train.to_csv('../data/dataset2_train.csv', index=False)\n",
    "dataset2_test.to_csv('../data/dataset2_test.csv', index=False)\n",
    "dataset2_train.to_parquet('../data/dataset2_train.parquet', index=False)\n",
    "dataset2_test.to_parquet('../data/dataset2_test.parquet', index=False)\n",
    "print(\"âœ“ Dataset 2 saved\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All datasets saved successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nFiles created in ../data/ directory:\")\n",
    "print(\"  - dataset1_train.csv / .parquet\")\n",
    "print(\"  - dataset1_test.csv / .parquet\")\n",
    "print(\"  - dataset2_train.csv / .parquet\")\n",
    "print(\"  - dataset2_test.csv / .parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET GENERATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. DATA COLLECTION\n",
      "   - Number of stocks: 2\n",
      "   - Date range: 2010-01-01 to 2024-12-31\n",
      "   - Tickers: AAPL, MSFT...\n",
      "\n",
      "2. DATASET 1 (Basic OHLCV)\n",
      "   Columns: ['Close', 'High', 'Low', 'Open', 'Volume', 'Target']\n",
      "   - Training: 6,042 rows, 6 features\n",
      "   - Testing:  1,504 rows, 6 features\n",
      "\n",
      "3. DATASET 2 (With Technical Indicators)\n",
      "   Columns: ['Close', 'High', 'Low', 'Open', 'Volume', 'Target', 'SMA-10', 'SMA-50', 'SMA-200', 'RSI', 'MACD', 'Signal_Line']\n",
      "   - Training: 5,644 rows, 12 features\n",
      "   - Testing:  1,504 rows, 12 features\n",
      "\n",
      "4. TECHNICAL INDICATORS INCLUDED\n",
      "   - SMA_10, SMA_50, SMA_200 (Simple Moving Averages)\n",
      "   - RSI_14 (Relative Strength Index)\n",
      "   - MACD, MACD_Signal, MACD_Histogram\n",
      "\n",
      "5. FILE OUTPUTS\n",
      "   CSV and Parquet formats saved to ../data/ directory\n",
      "\n",
      "================================================================================\n",
      "Dataset generation complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATASET GENERATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA COLLECTION\")\n",
    "print(f\"   - Number of stocks: {len(top_50_tickers)}\")\n",
    "print(f\"   - Date range: 2010-01-01 to 2024-12-31\")\n",
    "print(f\"   - Tickers: {', '.join(top_50_tickers[:10])}...\")\n",
    "\n",
    "print(\"\\n2. DATASET 1 (Basic OHLCV)\")\n",
    "print(f\"   Columns: {list(dataset1_train.columns)}\")\n",
    "print(f\"   - Training: {dataset1_train.shape[0]:,} rows, {dataset1_train.shape[1]} features\")\n",
    "print(f\"   - Testing:  {dataset1_test.shape[0]:,} rows, {dataset1_test.shape[1]} features\")\n",
    "\n",
    "print(\"\\n3. DATASET 2 (With Technical Indicators)\")\n",
    "print(f\"   Columns: {list(dataset2_train.columns)}\")\n",
    "print(f\"   - Training: {dataset2_train.shape[0]:,} rows, {dataset2_train.shape[1]} features\")\n",
    "print(f\"   - Testing:  {dataset2_test.shape[0]:,} rows, {dataset2_test.shape[1]} features\")\n",
    "\n",
    "print(\"\\n4. TECHNICAL INDICATORS INCLUDED\")\n",
    "print(\"   - SMA_10, SMA_50, SMA_200 (Simple Moving Averages)\")\n",
    "print(\"   - RSI_14 (Relative Strength Index)\")\n",
    "print(\"   - MACD, MACD_Signal, MACD_Histogram\")\n",
    "\n",
    "print(\"\\n5. FILE OUTPUTS\")\n",
    "print(\"   CSV and Parquet formats saved to ../data/ directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Dataset generation complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
